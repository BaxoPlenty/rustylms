# rustylms - A LM-Studio API wrapper written in Rust

> ℹ️ If you are looking for an ollama api wrapper, consider looking at [ollama-rs](https://github.com/pepperoni21/ollama-rs)

> ⚠️ This project is still not finished! Bugs may occur

This library provides support for [**LM Studio Servers**](https://lmstudio.ai). All features are made according to the [official documentation](https://lmstudio.ai/docs/local-server).

## Feature List

-   Generating completions using chats
-   Retrieving all models from the server

## To-Do List

-   ~~Generating completions~~
    -   Supporting streams as responses
-   Creating embeddings
