# rustylms - A LM-Studio API wrapper written in Rust

> ℹ️ If you are looking for an ollama api wrapper, consider looking at [ollama-rs](https://github.com/pepperoni21/ollama-rs)
> ⚠️ This project is still not finished! Bugs may occur

This library provides support for **LM Studio Servers**. All features are made while reading the [official documentation](https://lmstudio.ai/docs/local-server).

## Feature List
- Retrieving all models from the server

## To-Do List
- Generating completions
    - Supporting streams as responses
- Creating embeddings
